---
title: "Dimensionality reduction"
author: "Aya benh hriz"
date: "05/11/2021"
output:
  pdf_document: default
  word_document: default
---
## Importing libraries
```{r}
library('plyr')
library('dplyr')
library(Hmisc)
library("viridis")  
library('naniar')
library(xtable)
library(schoRsch)
library(gmodels)
library(readr)
library(ggpubr)
library(magrittr)
library(dplyr)
library("FactoMineR")
library("factoextra")
library(RColorBrewer)
library(scales)
```

```

## Importing our data set
```{r}
df  <- read.csv('users.db.csv')
head(df)
```
```{r}
df$gender=factor(df$gender)
```

```{r}
# correlation for all variables
corr=df[,c(3,4,5,6,11,12,13,14,15,16)]
tab=round(cor(corr),
  digits = 2 # rounded to 2 decimals
)
tab
#This correlation matrix gives an overview of the correlations for all combinations of two variables.
```

```{r}
library(corrplot)

corrplot(cor(corr),
  method = "number",
  type = "upper" # show only upper side
)
```
```{r}
# multiple scatterplots
pairs(df[, c("score", "n.matches", "n.updates.photo","sent.ana")])
#The figure indicates that score is positively correlated with n.matches, n.updates.photo and sent.ana. 
```

##dentifying correlations in the variables

```{r cars}
ggqqplot(df,x="score",color="gender",ylab = "score")
```

```{r}
ggqqplot(df,x='n.matches',color="gender",ylab = "n.matches")
#check if it's normal it doesn't follow a straight line so it's not normal and we use  then spearman's 
```


```{r}
tab=cor.test(df$score,df$n.matches, method = 'spearman')
```
```{r}
hist(df$score,col="chocolate")
```
```{r}
hist(log(df$score),col="blue")
```


```{r}
hist(df$n.matches,col="chocolate")
```


```{r}
hist(log(df$n.matches),col="blue")
```
```{r}
#when it's not normally distributed we have to use log so that we can use lm
mod <- lm(log(n.matches) ~ log(df$score)+gender+photo.keke+gender*photo.beach, data = df)
summary(mod)
```
### Dimensionality Reduction
## PCA 
```{r}
#keep only the nuemrical variables for scaling
df2=df[,c("score","n.matches","n.updates.photo","sent.ana")]
#inertia drops when you add more variables 
#when scaling we only keep the numeric variables
res.pca=PCA(df2, scale.unit = TRUE, graph = FALSE)
PCA(df2, scale.unit = TRUE, graph = TRUE)

```
```{r}
mt.pca <- prcomp(df2, center = TRUE,scale. = TRUE)

summary(mt.pca)

```









```{r}
fviz_pca_biplot(res.pca, 
                repel = TRUE, # Avoid text overlapping (slow if many point)
                ggtheme = theme_minimal(),select.ind=list(contrib=200))
```

```{r}
fviz_pca_ind(res.pca, 
                repel = TRUE, # Avoid text overlapping (slow if many point)
                ggtheme = theme_minimal(),select.ind=list(contrib=1000), 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),col.ind = "contrib")
#the further the obs is far from the origin for the x axis the highest the contirb for the 1st pr comp 
```

```{r}
fviz_screeplot(res.pca, addlabels = TRUE, ylim = c(0, 65))
```



## Apply pca and kmean
## using MCA for discrete variables
```{r}
df3=df[,c(13,14,15,16,10)]
for (i in 1:5) {
  plot(df3[,i], main=colnames(df3)[i],
       ylab = "Count", col="steelblue", las = 2)
}
```

```{r}
df3[sapply(df3, is.numeric)] <- lapply(df3[sapply(df3, is.numeric)], 
                                       as.character)
df3[sapply(df3, is.character)] <- lapply(df3[sapply(df3, is.character)], 
                                       as.factor)
MCA(df3)
```

```{r}
res.mca <- MCA(df3, graph = FALSE)
print(res.mca)
```
```{r}
fviz_screeplot(res.mca, addlabels = TRUE, ylim = c(0, 45))
```
```{r}
fviz_mca_biplot(res.mca, 
                repel = TRUE, # Avoid text overlapping (slow if many point)
                ggtheme = theme_minimal())

```

```{r}
#individuals_MCA
fviz_mca_var(res.mca, col.ind = "contrib", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE, # Avoid text overlapping (slow if many points)
             ggtheme = theme_minimal())
```

```{r}
wss <- (nrow(df2)-1)*sum(apply(df2,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(df2,
                                     centers=i)$withinss)
```


```{r}
plot(1:15, wss, type="b", xlab="Number of Clusters",
     ylab="Within groups sum of squares")
#So here we can see that the "elbow" in the scree plot is at k=4, so we apply the k-means clustering function with k = 4 and plot.
```

```{r}
# From scree plot elbow occurs at k = 4
# Apply k-means with k=4
pc <- prcomp(df2)
comp <- data.frame(pc$x[,1:2])
k <- kmeans(comp, 4, nstart=25, iter.max=1000)
```


```{r}
palette(alpha(brewer.pal(9,'Set1'), 0.5))
plot(comp, col=k$clust, pch=16)
```
## HCPC 
```{r}
res.pca <- PCA(df2, graph = FALSE)
res.hcpc <- HCPC(res.pca, graph = FALSE)
plot(res.hcpc,choice='tree')
```

```{r}
plot(res.hcpc)
```

